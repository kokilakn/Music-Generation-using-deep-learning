{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_midi(file):\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "  \n",
    "    #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Acoustic Bass' in str(part): \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "if (os.path.isfile(\"Acoustic Bass.csv\") == False):\n",
    "    path='/Users/kokilareddy/Downloads/midis/Guitar_midkar.com_MIDIRip/jazz/'\n",
    "\n",
    "    #read all the filenames\n",
    "    files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "    #reading each midi file\n",
    "    notes_array = np.array([read_midi(path+i) for i in files])\n",
    "    \n",
    "    import csv\n",
    "    with open(\"Acoustic Bass.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(notes_array)\n",
    "else:\n",
    "    data_2d = []\n",
    "    with open('Acoustic Bass.csv', newline='\\n') as f:\n",
    "        reader = csv.reader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            data_2d.append([x for x in row])\n",
    "    i=0;\n",
    "    for music in data_2d:\n",
    "        if (music):\n",
    "            data_2d[i] = music[0].split(\",\")\n",
    "        i+=1\n",
    "    notes_array = data_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658\n"
     ]
    }
   ],
   "source": [
    "#converting 2D array into 1D array\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "\n",
    "#No. of unique notes\n",
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library\n",
    "from collections import Counter\n",
    "\n",
    "#computing frequency of each note\n",
    "freq = dict(Counter(notes_))\n",
    "\n",
    "#library for visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#consider only the frequencies\n",
    "no=[count for _,count in freq.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n"
     ]
    }
   ],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_music=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp=[]\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)            \n",
    "    new_music.append(temp)\n",
    "    \n",
    "new_music = np.array(new_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
    "#preparing input sequences\n",
    "x_seq=[]\n",
    "for i in x:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "        #assigning unique integer to every note\n",
    "        temp.append(x_note_to_int[j])\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
    "y_seq=np.array([y_note_to_int[i] for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)\n",
    "x_tr = np.reshape(x_tr, (x_tr.shape[0], no_of_timesteps, 1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], no_of_timesteps, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128,return_sequences=True, input_shape = (x_tr.shape[1], 1)))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(len(unique_y)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 3.5084\n",
      "Epoch 00001: val_loss improved from inf to 3.24438, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 1227s 897ms/step - loss: 3.5084 - val_loss: 3.2444\n",
      "Epoch 2/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 3.1065\n",
      "Epoch 00002: val_loss improved from 3.24438 to 2.99363, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 1237s 904ms/step - loss: 3.1065 - val_loss: 2.9936\n",
      "Epoch 3/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.8838\n",
      "Epoch 00003: val_loss improved from 2.99363 to 2.83746, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 1228s 898ms/step - loss: 2.8838 - val_loss: 2.8375\n",
      "Epoch 4/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.7332\n",
      "Epoch 00004: val_loss improved from 2.83746 to 2.74002, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 1223s 894ms/step - loss: 2.7332 - val_loss: 2.7400\n",
      "Epoch 5/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.6157\n",
      "Epoch 00005: val_loss improved from 2.74002 to 2.66402, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 1278s 935ms/step - loss: 2.6157 - val_loss: 2.6640\n",
      "Epoch 6/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.5274\n",
      "Epoch 00006: val_loss improved from 2.66402 to 2.59320, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 1153s 843ms/step - loss: 2.5274 - val_loss: 2.5932\n",
      "Epoch 7/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.4519\n",
      "Epoch 00007: val_loss improved from 2.59320 to 2.56108, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 938s 686ms/step - loss: 2.4519 - val_loss: 2.5611\n",
      "Epoch 8/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.3874\n",
      "Epoch 00008: val_loss improved from 2.56108 to 2.52912, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 856s 625ms/step - loss: 2.3874 - val_loss: 2.5291\n",
      "Epoch 9/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.3297\n",
      "Epoch 00009: val_loss improved from 2.52912 to 2.50510, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 836s 611ms/step - loss: 2.3297 - val_loss: 2.5051\n",
      "Epoch 10/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.2765\n",
      "Epoch 00010: val_loss improved from 2.50510 to 2.50507, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 909s 664ms/step - loss: 2.2765 - val_loss: 2.5051\n",
      "Epoch 11/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.2304\n",
      "Epoch 00011: val_loss improved from 2.50507 to 2.46067, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 880s 644ms/step - loss: 2.2304 - val_loss: 2.4607\n",
      "Epoch 12/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.1851\n",
      "Epoch 00012: val_loss improved from 2.46067 to 2.45107, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 873s 638ms/step - loss: 2.1851 - val_loss: 2.4511\n",
      "Epoch 13/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.1436\n",
      "Epoch 00013: val_loss did not improve from 2.45107\n",
      "1368/1368 [==============================] - 734s 536ms/step - loss: 2.1436 - val_loss: 2.4518\n",
      "Epoch 14/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.1063\n",
      "Epoch 00014: val_loss improved from 2.45107 to 2.43858, saving model to Acoustic Bass.h5\n",
      "1368/1368 [==============================] - 732s 535ms/step - loss: 2.1063 - val_loss: 2.4386\n",
      "Epoch 15/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.0707\n",
      "Epoch 00015: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 769s 562ms/step - loss: 2.0707 - val_loss: 2.4454\n",
      "Epoch 16/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.0344\n",
      "Epoch 00016: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 720s 526ms/step - loss: 2.0344 - val_loss: 2.4521\n",
      "Epoch 17/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 2.0031\n",
      "Epoch 00017: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 514s 376ms/step - loss: 2.0031 - val_loss: 2.4451\n",
      "Epoch 18/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.9724\n",
      "Epoch 00018: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 504s 368ms/step - loss: 1.9724 - val_loss: 2.4530\n",
      "Epoch 19/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.9435\n",
      "Epoch 00019: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 478s 349ms/step - loss: 1.9435 - val_loss: 2.4483\n",
      "Epoch 20/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.9153\n",
      "Epoch 00020: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 478s 349ms/step - loss: 1.9153 - val_loss: 2.4801\n",
      "Epoch 21/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.8880\n",
      "Epoch 00021: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 492s 360ms/step - loss: 1.8880 - val_loss: 2.4901\n",
      "Epoch 22/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.8624\n",
      "Epoch 00022: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 481s 351ms/step - loss: 1.8624 - val_loss: 2.4743\n",
      "Epoch 23/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.8390\n",
      "Epoch 00023: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 519s 379ms/step - loss: 1.8390 - val_loss: 2.5026\n",
      "Epoch 24/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.8134\n",
      "Epoch 00024: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 495s 362ms/step - loss: 1.8134 - val_loss: 2.5129\n",
      "Epoch 25/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.7922\n",
      "Epoch 00025: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 493s 361ms/step - loss: 1.7922 - val_loss: 2.5123\n",
      "Epoch 26/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.7686\n",
      "Epoch 00026: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 498s 364ms/step - loss: 1.7686 - val_loss: 2.5247\n",
      "Epoch 27/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.7494\n",
      "Epoch 00027: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 476s 348ms/step - loss: 1.7494 - val_loss: 2.5392\n",
      "Epoch 28/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.7288\n",
      "Epoch 00028: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 504s 368ms/step - loss: 1.7288 - val_loss: 2.5623\n",
      "Epoch 29/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.7087\n",
      "Epoch 00029: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 504s 369ms/step - loss: 1.7087 - val_loss: 2.5650\n",
      "Epoch 30/30\n",
      "1368/1368 [==============================] - ETA: 0s - loss: 1.6882\n",
      "Epoch 00030: val_loss did not improve from 2.43858\n",
      "1368/1368 [==============================] - 483s 353ms/step - loss: 1.6882 - val_loss: 2.5731\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "if(os.path.isfile(\"Acoustic Bass.h5\") == False):\n",
    "    model=lstm()\n",
    "    mc=ModelCheckpoint('Acoustic Bass.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
    "    history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,\n",
    "                        epochs=30, validation_data=(np.array(x_val),np.array(y_val)),\n",
    "                        verbose=1, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best model\n",
    "from keras.models import load_model\n",
    "model = load_model('Acoustic Bass.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction():\n",
    "    import random\n",
    "    ind = np.random.randint(0,len(x_val)-1)\n",
    "\n",
    "    random_music = x_val[ind]\n",
    "\n",
    "    predictions=[]\n",
    "    for i in range(30):\n",
    "\n",
    "        random_music = random_music.reshape(1,no_of_timesteps,1)\n",
    "\n",
    "        prob  = model.predict(random_music)[0]\n",
    "        y_pred= np.argmax(prob,axis=0)\n",
    "        predictions.append(y_pred)\n",
    "\n",
    "        random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "        random_music = random_music[1:]\n",
    "\n",
    "    print(predictions)\n",
    "\n",
    "    x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
    "    predicted_notes = [x_int_to_note[i] for i in predictions]\n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output):\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.AcousticBass()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.AcousticBass()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "    output_notes.append(instrument.AcousticBass())\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='Acoustic Bass.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 128, 7, 151, 42, 128, 7, 42, 169, 169, 128, 4, 42, 166, 7, 169, 42, 169, 42, 169, 42, 42, 169, 42, 42, 169, 42, 42, 169, 42]\n"
     ]
    }
   ],
   "source": [
    "predicted_notes=prediction()\n",
    "convert_to_midi(predicted_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.call(['sh', 'AcousticBass.sh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
